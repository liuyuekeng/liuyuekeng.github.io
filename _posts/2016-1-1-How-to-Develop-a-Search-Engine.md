---
layout: post
title:  "《自制搜索引擎》"
date:   2016-1-1 17:08:00
categories: 书评
---

日本人似乎很钟爱DIY系列，自制操作系统，自制编程语言，甚至自制CPU，都是日本人写的。
感觉这个系列最后很可能会出一本自制女朋友之类的书。

这次买了一本刚出的书《自制搜索引擎》。
其实并没想过要真的自制一个搜索引擎，倒是对搜索引擎都在干些什么挺好奇的，可以一看。

###如何快速检索大量文本

全文检索平时搜代码的时候经常用的grep算是一种，
但当文本量很大的时候直接grep来扫描，显然是不科学的。
我们想要快速地检索大量的文本，自然而然地会想到使用索引来进行。
简单直接的空间换时间的思维。
书中介绍的索引结构是常见的倒排索引。

###索索引擎怎么做

简单来说，倒排索引就是用一种结构来存储各个词元在文档中出现的位置。

1. 在一开始收录文档的时候，先对文档进行分词，然后记录词元在哪些文档中出现，出现的位置都在哪里。
这个记录位置的文件就叫做倒排文件。

2. 后面进行检索的时候，也对query进行相同的分词，
然后在倒排文件中查找词元出现的位置，完成快速检索。

嗯，搜索引擎就干了两件事儿。

###其他

这就完啦？当然没有这样的好事。原理简单，实现却是复杂的。下面每一块要做好都涉及很多难题

* 如何分割词元？词素解析的准确率直接影响搜索结果，到底怎么分词是一个非常复杂的问题。
  一般来说搜索引擎会提供多种分词供检索提高召回率。
* 如何进行归一？全角，半角，编码，误输入等等的处理。
  这其中误输入的处理可以通过机器自主学习来进行（用户相邻相似的检索一般为误输入）
* 怎么构建倒排文件？海量文档的索引构建肯定没法在内存里直接完成，需要与二级存储合并。
  什么时候合并？怎么合并？一台机子性能不足的时候怎么做分布式？
* 搜索的时候如何快速查找索引？如何对搜索结果进行计分排序？
  排名这一点上有很多学问，既要能反映文档地相关性，又要防止恶意的SEO
* 分布式的系统怎么排序？如何使用缓存来加快搜索？
* 索引如何压缩？索引文件存储的内容就是文档id和偏移量（小整数），文件本身又很大，非常适合压缩。
  索引检索在内存中进行，索引非常大的时候需要多次分批从二级存储读取，很多时间消耗在IO上。
  压缩索引可以大大降低IO的负载，加快速度。
* 另外还有许许多多策略的调整，用来平衡准确率，召回率，查询速度等等指标。
* 等等等等。。。

当然还有如何获取文档的问题？实际上这并不属于搜索引擎的范畴，搜索引擎只管搜索。
但web搜索引擎离不开网页爬虫，而网页爬虫本身也是一个很复杂的问题。

这本书比想象中要薄得多（我还以为是一本很厚的书呢，拆快递的时候感觉亏了）。
内容尽量保持了简单，基本就是一个带逛的意思，这倒是很符合我一开始买它的目的。
示例中的代码使用的是c语言，但有详细的注解，不看代码也没什么大问题，领略一下就好。
