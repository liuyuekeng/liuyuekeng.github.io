---
layout: post
title:  "翻译《Purely Functional Data Structures》"
date:   2016-1-9 17:14:00
categories: [books, coding]
---

目录

<a href="#1">1 简介</a>

<a href="#1-1">1.1 函数式 VS. 指令式数据结构</a>

<a href="#1-2">1.2 严格计算 VS. 惰性计算</a>

<a href="#1-3">1.3 贡献</a>

<a href="#1-4">1.4 源码</a>

<a href="#1-5">1.5 术语</a>

<a href="#1-6">1.6 概览</a>

<a href="#2">2 惰性计算和$符</a>

<a href="#2-1">2.1 流</a>

<a href="#2-1">2.2 历史记录</a>

<a href="#3">3 使用惰性计算的摊销与不变性</a>

<a href="#3-1">3.1 传统的摊销</a>

<a href="#3-1-1">3.1.1 例子：队列</a>

<a href="#3-2">3.2 不变性：多重未来的问题</a>

<a href="#3-2-1">3.2.1 执行跟踪与逻辑时间</a>

<a href="#3-3">3.3 调和摊销分析和持久性</a>

<a href="#3-3-1">3.3.1 惰性计算的作用</a>

<a href="#3-3-2">3.3.2 一个用来分析惰性数据结构的框架</a>

<a href="3-4">3.4 银行家方法</a>

#<a name="1">1 简介</a>

对高效数据结构的研究已经超过30年，这些牛逼的程序员从各种复杂问题中提取出高效的解决方案，产生一揽子著作。
本来这些著作应该不依赖于编程语言的，但不幸的是，这些著作看上去更像是亨利福特意义上的
“程序员能够使用任意的编程语言，只要它指令式的（这里把imperative翻译成指令式，后面都是如此）”。
（亨利福特的名言：“[Customers] can have any colorthey want, as long as it’s black.”）
是只有少数的数据结构是适合在函数式语言中使用的，诸如Standard ML,Haqskell。
这本书就是要来帮助弱小的!思考，分析函数式数据结构的设计。

##<a name="1-1">1.1 函数式 VS. 指令式数据结构</a>

函数式编程方法论的优势已经为大众所知[Bac78, Hug89, HJ94]，但是绝大多数的程序仍然是用指令式编程语言（C语言之类）写的。
这种明显的矛盾可以轻易地用函数式语言起步比较晚来解释，但这个差距也在逐渐缩小。
函数式语言从基础编译技术，到复杂的分析和优化，各个方面都有了长足的进步。
然而，函数式编程还有一块短板，没多人能够用上合适的数据结构。
不幸的是，没多少书可以在这方面提供有用的参考。

为什么函数式的数据结构比起指令式的数据结构，更难设计和实现呢？
主要是两个最基本的问题。

第一个问题是，函数式编程强烈反对破坏性的更新（事务），这点非常难搞，相当于没收了大厨的刀子。
其实破坏性的更新就像刀子，可以很危险，也可以很高效。
指令式的数据结构本身极度依赖事务，所以函数式编程只能另谋出路。

第二个问题是，函数式数据结构需要比指令式的数据结构更加灵活。
尤其是，当我们更新一个指令式的数据结构的时候，往往认为旧版本的数据可以不要了，
但更新一个函数式的数据结构的时候，我们希望新的和旧的数据都支持访问，来实现更多的功能。
一个数据结构支持多版本访问，我们称之为持久化（persistent），
反之同时支持持单版本访问的称做非持久化（好吧，这是我随便起的，后面都这么翻译的，原文是ephemeral）[DSST89]。
函数式编程语言有一个特点，所有数据结构生来就是持久化的。
指令式的数据结构基本上都是非持久化的，但是，如果指令式编程语言需要一个持久化的数据结构时，
理所当然地这个数据结构会比较复杂，而且相比起来效率更低。

另外，理论家已经至少证明，在一些情况下，函数式编程语言会比指令式编程语言运行效率低[BAG92, Pip96]
对于这些观点，这本书要打他们的脸，性能逼近非持久化数据结构的函数式数据结构往往是可以搞得出来的。

##<a name="1-2">1.2 严格计算 VS. 惰性计算</a>

大多数（顺序的）函数式语言可以根据他们求值的顺序，被归为严格计算和惰性计算两类。
这是一个一直被函数式程序员们虔诚热烈讨论的话题。
两者最明显的区别莫过于他们如何处理函数的参数。
在严格计算的语言中，参数在函数体执行前被求值。
而在惰性计算语言中，参数是在需要的时候才被求值的。
他们在一开始传递的时候是未被赋值的，当且仅当计算需要时才被求值。
另外，一旦参数被赋值，参数的值就被保存起来了，一旦再次需要该值的时候，就可以不经过重复计算直接找到。
这个保存的行为被称为记忆[Mic68]。

不同的求值顺序有各自的优缺点，但严格计算模式至少有一个显而易见的优势：
减轻了渐进复杂度（asymptotic complexity）。
在严格计算的语言中，能够准确说出任意表达式何时会被求值，在大部分情况下更加显然。
因此，推测给定程序的运行情况也就更直观。
然而，在惰性计算的语言中，即便是老手也很难预测一个表达式什么时候，甚至是否会被求值。
使用这种语言的程序员常常不得不假装语言本身是严格的，来估算总运算时间。

两种求值方式都间接地影响着数据结构的设计和分析。
正如我们将要在第三、四章中看到的，严格计算的语言能够描述最坏情况的数据结构，
却不能描述摊销的数据结构，而惰性计算语言能够描述摊销数据结构，却不能表示最坏情况的数据结构。
为了能够描述这两种数据结构，我们需要一种支持两种求值顺序的语言。
幸运的是，将两者融合到一个语言中并不难。
第二章介绍了$符，往严格计算语言中添加惰性计算的方法（以Standar ML为例子）

##<a name="1-3">1.3 贡献</a>

参考了这些文献，这就不翻译了吧

* Functional programming. Besides developing a suite of efficient data structures that
are useful in their own right (see Table 1.1), we also describe general approaches to
designing and analyzing functional data structures, including powerful new techniques
for reasoning about the running time of lazy programs.

* Persistent data structures. Until this research, it was widely believed that amortization
was incompatible with persistence [DST94, Ram92]. However, we show that memoization,
in the form of lazy evaluation, is the key to reconciling the two. Furthermore, as
noted by Kaplan and Tarjan [KT96b], functional programming is a convenient medium
for developing new persistent data structures, even when the data structure will eventually
be implemented in an imperative language. The data structures and techniques in
this thesis can easily be adapted to imperative languages for those situations when an
imperative programmer needs a persistent data structure.

* Programming language design. Functional programmers have long debated the relative
merits of strict and lazy evaluation. This thesis shows that both are algorithmically important
and suggests that the ideal functional language should seamlessly integrate both.
As a modest step in this direction, we propose $-notation, which allows the use of lazy
evaluation in a strict language with a minimum of syntactic overhead.

##<a name="1-4">1.4 源码</a>

所有的源码都用Standard ML写的[MTH90]，提供原始的惰性计算。
而且能够方便地转化成任何支持严格和惰性计算的函数式编程语言。
对那些使用纯严格计算或纯惰性计算函数式语言的程序员来说，就只有部分的数据能用得上。

在第七章第八章中，我们将会遇到一些递归的数据结构，用Standard ML很难描述清楚。
由于语言的限制，对于某些复杂的递归形式，比如多态递归和递归模块。
当这种情况出现的时候，我们会牺牲可执行性，使用类似Standard ML的伪代码来将递归形式描述清楚。
然后我们才来看怎么实现成合法的Standard ML。
这些例子可以被看做对编程语言设计界提供具有简单抽象能力语言的挑战。

##<a name="1-5">1.5 术语</a>

任何关于数据结构的讨论都会充满歧义，毕竟数据结构这个词就有至少四中相关的有意义的释义。

* 一个抽象数据类型（一个类和它的方法的集合）。我们将它当成一个抽象

* 一个具体抽象数据类型的实现。我们把它当成一个实现。
但要注意，一个实现不一定非要是代码实现，一个实现的设计也算。

* 一个数据类型的实例。比如某种列表或某种树。
我们将它当做一个实例，通常来说是一个对象或者一个版本。
然而，特定的数据类型通常有自己的名称。
举个例子，我们将简单地把一个栈或队列对象当做叫做栈，或队列。

* 一个不变的标识是在更新中保持不变。
比如说，一个基于栈的解析器，我们经常随口讲“the stack”，就好像只有一个唯一的栈，
而不是在不同的时间存在不同的版本。
我们将这种标识当做永恒不变的。、
这种情况主要出现在我们讨论具有不变性的数据结构的时候，
我们所说的同一个数据结构的不同版本，其实是指不同版本共享一个共同的不变标识

Roughly speaking, abstractions correspond to signatures in Standard ML, implementations
to structures or functors, and objects or versions to values. There is no good analogue for
persistent identities in Standard ML.（妈的，这段实在不知道是什么鬼）

类似地，operation这个词是被重载的，表示一个抽象数据类型支持的功能，和这些功能的实现。
我们的operation指的是第二个含义，对于第一个含义我们使用operator和function做代替。

##<a name="1-6">1.6 概览</a>

这本书包括两个部分。第一个部分（第二章到第四章）内容涉及算法方面的惰性计算。
第二章简要回顾了懒惰计算的基本概念，引入了$运算符。

第三章是这本书余下部分的基础。它描述了惰性运算在结合摊销和持久化中的地位，
而且给定了两种分析惰性计算摊销的消耗的方法。

第四章说明了在一种语言中结合严格计算和惰性计算的力量。
它描述了通过系统地调度惰性组件提前执行，常常能够在摊销数据结构中提取出最坏情况的数据结构。

本书的第二部分（从第五章到第八章）涉及函数式数据结构的设计。
记载适合每一种目的的高效数据结构是一个无法完成的任务，
所以我们把心思集中在几种常见的设计高效函数式数据结构的技术，
并列举每一种技术的一个或几个基本抽象的实现。
比如优先队列，随机存取结构，和各种序列。

第五章描述了惰性重建，这是全局重建的一种惰性变形[Ove83]。
惰性重建比全局重建明显简单，但是产生摊销约束而不是最坏情况约束。
结合惰性重建和第四章中的调度技术，最坏情况约束可以被找出来。

第六章探讨了数字的表示，实现了类似数字表示的设计（通常是二进制数字）。
在这个模型中，设计高效的插入和删除操作，相当于选择增加或减少常数时间的二进制数字变体。

第七章考察数据结构的引导[Buc93]。数据结构引导有两种：
结构分解，无限的解决方案通过有限的解决方案引导而来；
结构抽象，高效的解决方案通过抵消的解决方案引导而来。

第八章介绍了隐含的递归减缓，Kaplan and Tarjan [KT95]递归减缓技术的惰性变种。
就惰性重建而言，隐含的递归减缓要更为简单，但是收获摊销而不是下界。
同样的，我们可以通过调度来解决下界的问题。

最后，第九章通过持久化数据结构，编程语言设计，和一些公开问题的讨论，
总结了上述工作对函数式编程的影响。

#2<a name="2">惰性计算和$符</a>

惰性计算是一种被许多纯函数式语言采纳的计算策略，比如Haskell [H+92]。
这种计算策略有两个重要性质。
第一，给定表达式的计算会被推迟或中止，直到这个计算结果被需要。
第二，一旦一个被中止的表达式被求值，这个值会被记忆（即缓存），
所以下一次这个值被需要时，就能直接查找避免重复计算。

在严格计算语言中支持惰性计算（比如Standard ML）需要两个原语：
一个用来中止表达式的求值，另一个用来恢复被中止的表达式求值（并记忆求值的结果）。
这些原语经常被称为“延迟”和“推动”（这里用把force翻译为推动）。
举个例子，在新泽西州的Standard ML中提供了下面的原语来支持惰性求值

![2-1](/images/Functional-Data-Structures-2-1.png)

这些原语足以将本书中所有算法进行编码。
然而使用这些原语编程是很不方便的。
举个例子，要中止一个表达式<code>e</code>的求值，
要写`delay (fn () => e)`这么长一串。
根据用多少个空白符，前缀可以达到13-17个字符！
虽然当只有少数表达式要被中止的时候，这是可以接受的，
但当有很多表达式需要被延迟的时候，这简直不能忍。

为了让延迟表达式语法不那么重，我们引入了$符来中止表达式求值。
要中止表达式`e`。我们直接写<code>$e</code>就好了。
`$e`被称为一个中止，拥有一个叫做`τ susp`的类型，
其中`τ`表示`e`的类型。
$符的作用域会尽可能向右延伸。
因此，`$f x`会被当做`$(f x)`而不是`($f) x`来解析,
`$x+y`会被当做`$(x+y)`而不是`($x)+y`。
要注意的是，`$e`本身也是一个表达式，所以也可以像`$$e`这样被中止。
得到一个嵌套延迟类型`τ susp susp`

设`s`是一个中止，类型是`τ susp`，那么`force s`计算并记忆`s`的内容，并返回类型为`τ`的结果。
然而，显式地用`force`来推动一个中止也是很麻烦的。
特别地，这常常对模式匹配造成坏的影响，
会需要用`force`操作，把一个表达式拆成两个或更多个部分。
为了避免这样的问题，我们将$符与模式匹配一起引入。
在`$p`这种形式之前，匹配一个模式，会先推动中止解决，然后再在`p`之前匹配这个结果。
有些时候，一个显式的`force`操作符仍然是相当有用的。然而现在，它能用$的形式来定义。

    fun force ($x) = x

为了对比两个符号，考虑一个标准的`take` 函数，功能是取得一个流的前n个元素。
流的定义如下：

![2-2](/images/Functional-Data-Structures-2-2.png)

使用`delay`和`force`写的`take`函数会长成这样

![2-3](/images/Functional-Data-Structures-2-3.png)


译者：如果你也和我一样没有见过ML语言，估计就懵逼了
戳这[wikipedia](https://en.wikipedia.org/wiki/Standard_ML)补补课吧，不谢。

相反，如果用上$运算符就可以写得更简洁些

![2-4](/images/Functional-Data-Structures-2-4.png)

实际上，他还可以写成这样酷酷的形式

![2-5](/images/Functional-Data-Structures-2-5.png)

然而，这第三种实现和前两种并不相等。
它在绑定的时候就执行了第二个参数的运算，而不是等到求结果流的时候

$运算符语法和语义的正是定义在附录A中

##<a name="2-1">2.1 Streams</a>

我们来写一个流，作为惰性求值和$运算符的一个拓展例子。
这个流也会在后面章节的一些数据结构中被用到。

流（也被称为惰性列表）与普通列表很相似，除了列表中的每一项都被系统地中止了。

![2-6](/images/Functional-Data-Structures-2-6.png)

一个简单的包含元素1,2,3的流可以写成

    $Cons (1, $Cons (2, $Cons (3, $Nil)))

把流和简单的类型为`α list susp`的被中止的列表放在一起比较是很有意义的。
后者的计算表现为一旦开始就一次性执行完毕，
另一方面，前者的要多次得推动stream执行恰好足够的计算，来得到外层的结果，并中止剩下的部分。
这种行为模式经常出现在，像这种包含嵌套中止的流等数据类型中。

为了更清楚的认识到这种行为的差异，考虑下自增函数`s ++ t`。在中止列表中，可能会被写成

    fun s ++ t = $(force s @ force t )

一旦开始，这个函数就推动两个参数执行然后拼接他们产生了整个结果。
因此，这个函数是不可分割的。
在流中，`append`函数会被写成这样

![2-7](/images/Functional-Data-Structures-2-7.png)

一开始，这个函数推动`s`的第一个元素（通过匹配$模式）。
如果这个部分是Nil，那么结果的第一个元素就是`t`的第一个元素，所以函数函数推动`t`。
反之，函数用`s`的第一个元素来构造结果的第一个元素*--这里是关键--*。
这个中止最终会计算附加列表的剩余部分。
因此，这个函数是增量的。前面描述的`take`函数也是类似地增量的。

然而，我们来考虑下去掉流的前n个元素的函数

![2-8](/images/Functional-Data-Structures-2-8.png)

这个函数是不可分割的，因为对`drop`的递归调用从来不会被延迟————
计算结果的第一个元素需要执行一整个`drop`函数。另一个不可分割的流函数是`reverse`。

![2-9](/images/Functional-Data-Structures-2-9.png)

这里对`reverse`的递归调用也没有被延迟过，
但要注意的是每次递归调用创建一个新的`$r`形式的中止。
这看起来可能会觉得`reverse`函数实际上没有一次性做完它的所有工作。
然而，像这种内容是明确的值的中止（也就是说完全由值和构造器组成，没有函数执行），
被称为平凡的。
一个合格的编译器会用早就记忆的形式来创建这些中止，
但就算编译器没有执行这种优化，平凡的中止总能在`O(1)`的时间内被解决。

虽然像`drop`和`reverse`这种不可分割的流函数很常见，
但是像`++`和`take`这种增量的函数在流中更有价值。
每一个中止处理一个微小但是重要的步骤，
所以为了效率最大化，懒惰执行只有在有必要的时候才应该被使用。
如果在惰性列表中只用到了不可分割的方法，那么应该使用简单的中止列表而不是流来实现。

![Figure 2.1](/images/Functional-Data-Structures-2-10.png)

Figure 2.1: A small streams package.

Figure 2.1用Standard ML总结了这些流函数。要注意流的类型定义是用Standard ML带类型类型的构造，
但是老版本的Standard ML不支持在签名中使用带类型的声明。
这个属性会在后面的版本中被支持，但如果编译器不允许，
那么简单的绕过方法是，删除流类型，并把每一个出现的`τ Stream`替换成`τ StreamCell susp`。
通过在签名中引入`StreamCell`数据类型，我们故意得选择暴露内部表示来支持流的模式。

##<a name="2-2">2.2 历史记录</a>

惰性计算，由Wadsworth [Wad71] 首次提出，作为减少lambda运算的优化。
Vuillemin [Vui74] 随后表明，惰性计算在某些特定条件下，是最优的计算策略。
惰性计算的正式语义已经被广泛地研究 [Jos89, Lau93, OLT94, AFM+95]。

Streams Landin 提出了流 [Lan65]，但没有涉及记忆。
Friedman，Wise [FW76] ，Henderson和Morris [HM76]将它扩展成记忆的形式

Memoization Michie [Mic68]创造了`memoization`这个词来表示带参数-结果对缓存的函数参数。
（参数字段减少，当把中止当做无参函数来记忆中止）。
Hughes [Hug85]随后将Michie原始意义上的`memoization`投入函数式编程的使用。

构成惰性求值-延迟计算和记忆结果的算法在算法设计中有很长的历史，虽然不总是一起出现。
延迟潜在昂贵计算的操作（通常是删除），
在哈希表 [WV86], 优先队列 [ST86b, FT87]和搜索树 [DSST89]中表现良好。
另一方面，记忆是诸如动态规划[Bel57]和路径压缩 [HU73, TvL84]等技术的的基本原理。

惰性计算在CAML [W+90]早期版本的语法，和Standard ML很相似，
为惰性求值提供了类似$符的支持，而不是提供一个惰性构造器，
然而，CAML允许任何数据构造器被标记为惰性，
之后所有的构造器实例都会被惰性求值。
虽然这样比起$更加灵活，单页造成程序难以阅读。
用$符可以在语法上明显地分辨出哪些自表达式会被严格计算，哪些会被惰性计算，
但在CAML中，这个信息只能由类型声明指定。

#<a name="3">3 使用惰性计算的摊销与不变性</a>

在过去的十五年，摊销已经成为设计与分析数据结构的强大工具。
拥有好的摊销约束的实现，往往比拥有等价的最坏情况约束的实现，来得简单高效。
不幸的是，标准的摊销技术只能在非持久化的数据结构中使用，
所以并不适合用来设计或分析这一章中原生的持久化数据结构。
我们来总结一下银行家模型和物理模型，这两种分析摊销数据结构的传统方法，
并找出他们在持久化数据结构中哪一个环节行不通。
然后，我们来证明惰性求值如何能调停摊销与持久化之间的冲突。
最后，我们改造银行家模型和物理模型来分析惰性摊销数据结构。
由此产生的技术，既是第一个用于设计与分析持久化摊销数据结构的技术，
也是第一个可用于分析非平凡的惰性程序的技术。

##<a name="3-1">3.1 传统的摊销</a>

摊销的概念是来自以下的观察。
给定一个操作序列，我们可能希望知道整个序列的运行时间，
而不关心其中任何一个独立操作的运行时间。
举个例子，给定n个操作的一个序列，我们可能希望用`O(n)`约束整个序列的运行时间，
并且不限定每一个操作都要约束在`O(1)`时间内。
我们可能会容忍几个操作开销在`O(log n)`甚至`O(n)`时间，只要最后整个操作的开销是`O(n)`就好。

这种自由为可能的解决方案提供了广阔的设计空间，
而且经常能产生与最坏情况解决方案约束等价的，更简单更高效的方案。
实际上，对于某些问题，比如并查找 [TvL84] 来说，
存在比可能最坏情况解决方案，更快收敛（这里把asymptotically faster翻译成更快收敛，不知道对不对）
的解决方案（在一些限制下） [Blu86]。

为了证明摊销的约束，定义每一个操作的摊销花费，
然后证明，任意的操作序列，所有操作的总摊销开销是总实际开销的上界。
也就是说。

![3-1](/images/Functional-Data-Structures-3-1.png)

其中 a<sub>i</sub> 是操作 i 的摊销成本， t<sub>i</sub> 是操作 i 的实际成本，
m是总操作数。实际上，通常一个稍微强一些的结果是：
对操作序列的任意中间阶段来说，累计的摊销成本是累计实际成本的上界。
也即是说。

![3-2](/images/Functional-Data-Structures-3-2.png)

对任意的 j 来说，累计摊销成本和累计实际成本之间的差被称为累计储蓄（accumulated savings，实在不知道是什么意思）。
因此，当累计储蓄非负的时候，累计摊销成本是累计实际成本的上界。

摊销允许少数操作的实际成本超过它的摊销成本。
这些操作被称为昂贵操作，而实际成本比摊销成本低的被称为廉价操作。
昂贵操作减少累计储蓄，廉价操作使其增加。
证明摊销约束的关键在于，证明昂贵操作只有在累计储蓄足够支付的时候出现，
不然累计储蓄就变成负的了。

Tarjan [Tar85]描述了两种分析短暂性摊销数据结构的方法：
银行家方法和物理方法。
在银行家方法中，累计储蓄被当成是数据结构中的单位位置的分数。
这些分数将会用来支付未来对这些位置的访问。
每一个操作的摊销成本被定义成，操作的实际成本加上所分配的分数，再减去操作花掉的分数。

![3-3](/images/Functional-Data-Structures-3-3.png)

c<sub>i</sub> 是分配给操作 i 的分数，
<span style="TEXT-DECORATION: overline">c</span><sub>i</sub>是操作 i 花去的分数。
分数必须在使用前被分配，而且也不能被重复使用。
所以，&sum;c<sub>i</sub> >= &sum;<span style="TEXT-DECORATION: overline">c</span><sub>i</sub> ，
从而保证 &sum;a<sub>i</sub> >= &sum;t<sub>i</sub>。
使用银行家方法的证明一般会定义一个不变的分数，
然后在每一个可能出现昂贵操作的地方，提前分配够用的分数。

在物理方法中，定义一个函数 Φ ，每一个对象 d 对应一个数值，称为 d 的势能。
函数 Φ 被精心构造出来，使得势能的初始值为0，而且永远非负。
势能代表累计储蓄的下界。

设 d<sub>i</sub>为操作 i 的输出，同时也是操作 i+1 的输入。
那么操作 i 的摊销成本被定义为实际成本加上潜力 d<sub>i-i</sub> 与 d<sub>i</sub>的差值。

![3-4](/images/Functional-Data-Structures-3-4.png)

操作序列的累计实际成本就是

![3-5](/images/Functional-Data-Structures-3-5.png)

&sum;(Φ(d<sub>i-1</sub>) - Φ(d<sub>i</sub>))的和，
正负交替出现相互抵消，被称为伸缩序列。
如果 Φ 被构造成 Φ(d<sub>0</sub>)为0， Φ(d<sub>j</sub>)为非负，
那么， Φ(d<sub>j</sub>) >= Φ(d<sub>0</sub>)，并且 &sum;a<sub>i</sub> >= &sum;t<sub>i</sub>，
就证明了累计的摊销成本是累计实际成本的上界。

注：这是物理方法的一个简单观点。
在实际分析中，常常出现很难放到上面所描述的框架里的情况。
比如，函数输入或输出的对象不止一个？
然而，这种简化的观点足以说明一些相关问题。

显然，这两种方法是相似的。
我们可以通过忽略位置，并把势能设成分数不变量，把银行家方法转换成物理方法。
相似的，我们也可以通过把势能转化成分数，并把所有分数放在根节点来把物理方法转化为银行家方法。
可能让人惊讶的是，银行家方法中的位置信息没有提供任何额外的能力，但两种方法实际上是等价的 [Tar85, Sch92]。
物理方法通常比较简单，但某些情况下，考虑位置反而更方便。

要注意，分数和势能只是分析工具，两者都不会实际出现在程序中（注释除外）

###<a name="3-1-1">3.1.1 例子：队列</a>

接下来我们通过分析一个队列抽象的简单函数式实现，来说明银行家方法和物理方法
如图Figure 3.1所示。

![Figure 3.1](/images/Functional-Data-Structures-3-6.png)

Figure 3.1: Signature for queues.

(Etymological note: snoc is cons spelled backward and means “cons on the right”.)

纯函数式队列 [Gri81, HM81, Bur82] 通常表现为一对列表，
`F` 和 `R`，`F` 包含正序排列的队列前面的元素，`R` 包含倒序排列的队列后面的元素。
比如包含整数1到6的队列，可能会被表现为列表 `F = [1, 2, 3]` 和 `R = [6, 5, 4]`。
这种表示被写成如下数据类型：

![3-7](/images/Functional-Data-Structures-3-7.png)

在这种形式中，队列的头就是 `F` 的第一个元素，所以 `head` 和 `tail` 各自返回并删除这个元素。

![3-8](/images/Functional-Data-Structures-3-8.png)

备注：为了避免读者为琐碎的细节分心，我们列举代码片段的时候通常会忽略错误情况。
比如，上面的代码片段不描述头或尾为空的列表的行为。
我们总会在完整的实现中来包含这些错误情况。

现在，列表的最后一个元素就是 `R` 的第一个元素， 
所以 `snoc` 只是简单地向 `R` 的头添加一个元素。

![3-9](/images/Functional-Data-Structures-3-9.png)

元素被添加到 `R` 中，从 `F` 中被删除，所以他们必须以某种方式从一个列表转移到另一个列表中。
这个过程是这么完成的，当 `F` 为空的时候，把 `R` 翻转，当做新的 `F` ,然后把 `R` 设为 `[]`。
这样做的目的是要保持不变式：`F` 只有当 `R` 也为空的时候，才能为空。
（也就是说整个队列是空的）。
要知道，如果 `F` 为空而 `R` 不为空的话，队列的第一个元素就是 `R` 的最后一个元素了，
这样访问时间就会变成 `O(n)` 了。
通过保持这个不变式，我们总能够在 `O(1)` 的时间内访问到队列的第一个元素。

`snoc` 和 `tail` 现在必须检测那些会破坏不变式的情况，并对应地改变他们的行为。

![3-10](/images/Functional-Data-Structures-3-10.png)

注意在 `snoc` 的第一个分句中使用的通配符(...)。
这是Standard ML中的模式匹配，代表“这个符号余下的部分是不相关的”。
在这个情况中，`R` 的值是无关的，因为我们知道 `F` 是 `[]`，那 `R` 也一定是空的。

编写这些函数的更简洁的方法是，
把 `snoc` 和 `tail` 的不变式保持合并到一个伪构造函数中（这里把 pseudo-constructor 翻译成伪构造函数）。
伪构造函数有时候也被称为智能构造函数（smart constructors） [[Ada93]，
是一个在数据构造中代替原构造函数的函数，
但是检查和执行是不变的。
在这个情况下，伪构造函数 `queue` 代理原构造函数 `Queue` 但也保证 `F` 只有在 `R` 也为空的时候才能为空。

![3-11](/images/Functional-Data-Structures-3-11.png)

完整的代码在图 Figure 3.2 中展示，除了 `tail` 函数在最坏情况下消耗 `O(n)` 的时间，
其他所有函数最坏情况下都消耗 `O(1)` 时间。
然而我们可以看到 `snoc` 和 `tail` 函数，
无论使用银行家方法还是物理方法分析，的摊销时间成本都只有 `O(1)`。

![3-12](/images/Functional-Data-Structures-3-12.png)

Figure 3.2: A common implementation of purely functional queues [Gri81, HM81, Bur82].

使用银行家方法，我们将维持一个不变量，整个尾部列表总会包含于它的长度相等的分数。
每一个对非空队列的 `snoc` 操作将消耗一个实际步骤，并给尾部列表里的新元素分配一分，
摊销成本为二。
每一个不涉及列表翻转的 `tail` 操作，消耗一个实际步骤，但不消耗分数也不分配分数，
摊销成本为一。
最后，那些翻转了尾部列表的 `tail` 操作花费 `m + 1` 个实际步骤，
其中 `m` 是尾部列表的长度，并花费了列表 `m` 个分数，所以摊销成本是 `m + 1 - m = 1`。

在这个简单的例子中，证明是几乎相同的。
即便如此，物理方法还是略微简单一些，原因如下。
使用银行家方法的时候，我们必须首先选择一个分数不变量，
然后描述整个函数在哪儿分配或花费分数。
分数不变量在其中起知道作用，但是一点都不自动。
例如， `snoc` 操作应该分配一个分数而不花费，还是分配两个分数而花费一个？
这两种做法的净收益是一样的，这个自由只是造成现在混乱的又一个源头。
另一方面，使用物理方法的话，我们只有一个决定要做————选择一个势能函数。
总之，分析只是计算而已，没有更多选择的自由。

##<a name="3-2">3.2 不变性：多重未来的问题</a>

在上面的分析中，我们假设队列的非持久化的（比如单线程的方式）。
那么如果我们想使用持久化的队列，会发生什么呢？

假设 `q` 是将 `n` 个元素插入到初始化为空的队列的结果，
那么 `q` 的头部列表将包含一个元素，而尾部列表包含 `n - 1` 个元素。
现在，假设我们通过取尾部 `n` 次来使用持久化的 `q`。
每一次调用 `tail` ， `q` 执行 `n` 个实际步骤。
那么这一系列操作的实际成本，包括建立 `q` 的时间，是 `n^2 + n`。
如果每个操作的摊销成本真的是 `O(1)` ，那么整体的实际成本应该是 `O(n)` 才对。
显然，使用序列的不变性破坏了上面所说的 `O(1)` 摊销成本的约束。
哪里出了问题呢？

在两种情况下，一个分析的基本要求被不变性数据结构破坏了。
银行家方法要求，分数不能被重复地使用，
物理方法要求一个操作的输出要作为下一个操作的输入
（或者更通常地说，一个输出不能被多次当做输入）。
现在，考虑下上面例子中第二个对 `tail q` 的调用。
第一次调用 `tail q` 把 `q` 尾部列表的所有分数都花光了，
没有分数可以用来支付接下来的调用，
所以银行家方法不管用。
而且第二次调用 `tail q` 需要的是 `q` 而不是第一次抵用的输出，所以物理方法也行不通。

上面两个问题说明了任何基于累计储蓄的记账系统的固有弱点————存起来的分数只能被花费一次。
传统的摊销方法依靠为未来的开销累计储蓄运转起来（无论是分数还是势能）。
在暂时性的情况下是没有问题的，因为每一个操作只有一个符合逻辑的未来状态。
但在不变性的情况下，一个操作可能会有多重逻辑未来（multiple logical futures），
每一个相互竞争，都要花掉相同的储蓄值。

###<a name="3-2-1">3.2.1 执行跟踪与逻辑时间</a>

我们所说的“逻辑未来”实际上是什么意思？

我们通过“执行跟踪”来为逻辑时间建立模型，提供一种对计算历史的抽象观察。
执行跟踪是一个有向图，其中节点代表我们感兴趣的操作，
通常是数据类型的更新操作。
一条从 `v` 指向 `v'` 的边则代表操作 `v'` 用到了操作 `v` 的一些结果。
操作 `v` 的“逻辑历史”记为 `^v` ，代表 `v` 的结果依赖的所有操作（包括 `v`本身）。
换句话说 `^v` 是所有节点 `w` 的集合，其中 `w` 必须存在从 `w` 到 `v` 的路径（长度可能为0）。
节点 `v` 逻辑未来是任意从 `v` 开始的路径所到达的终止节点（一个出度为0的节点）。
如果存在多条这样的路径，那么节点 `v` 就有多重逻辑未来。
我们有时候会提及一个对象的逻辑历史或逻辑未来，这表示创建该对象的操作的逻辑历史或逻辑未来。

执行追踪形成了版本图的概念 [DSST89]，经常被用来作为持久化数据结构历史模型。
在版本图中，节点代表了一个持久化标识的各个版本，边则代表多个版本之间的依赖关系。
所以，版本图是操作的结果模型，执行追踪则是操作本身的模型。
执行跟踪可以用来结合多个持久化标识（甚至可能不是同一个数据类型）的历史，
或者推理不返回新版本的操作（比如查询），或者返回多个结果的操作（比如把一个列表分成两个子列表）

对于非持久化数据结构来说，在版本图或者执行追踪中，每一个节点的出度被限制为不超过1，
反应每一个对象最多只能被更新一次。
为了得出多版本持久化数据的模型，版本图允许每一个节点的出度是无限的，但是要加上其他的限制。
比如说，版本图经常通过限制节点入度最多为一，来限制图形为树（或森林）。
另一些版本图允许入度比一大，但是禁止环的形成，确保每一个图都是一个有向无环图图。
我们对执行追踪不做这些限制。
入度比一大的节点对应那些需要多个参数的操作，比如列表连接或集合合并。
环在递归定义对象中出现，许多惰性计算支持这么搞。我们甚至允许在一对节点中存在多条边，
比如一个列表与自己连接。

我们将在3.4.1中使用执行追踪，用来拓展银行家方法来处理持久型数据结构

##<a name="3-3">3.3 调和摊销分析和持久性</a>

在前面的章节中，我们看到传统的摊销方法在持久化的时候行不通，因为他们假设只存在单一的未来，
也就是说累计储蓄只能最多被使用一次。
然而，在持久化系统中，多重逻辑未来可能都试图花掉这同一份储蓄。
在这一届中，我们展示银行家方法和物理方法如何才能，通过用累计债务代替累计储蓄的概念来纠正。
债务用来度量未被求值的惰性计算成本。
直觉上来说就是，虽然储蓄只能被花一次，但是多次偿还债务还想不会造成什么损失。

###<a name="3-3-1">3.3.1 惰性计算的作用</a>

要记得，昂贵操作是那些实际成本比（预期）摊销成本高的操作。
举个例子，假设程序 `f x` 是昂贵的。
在持久化的情况下，一个恶意的对手可能任意频繁地调用 `f x`
（注意每一个操作都是 `x` 的逻辑未来。）
如果每一个操作要花费相同的时间，那么摊销约束就降低为最坏情况约束了。
因此，我们必须找到一个方法来保证，如果第一个 `f` 操作对 `x` 是昂贵的，
那么接下来的 `f` 操作对 `x` 将不是昂贵的。

在值调用（call-by-value）（比如严格计算）或者按名称调用(call-by-name)（比如不带记忆的惰性计算）的情况下，
是不可能没有副作用的，因为每一个对 `x` 的`f`操作将花费恰好相同的时间。
因此，在只支持上述求值顺序的语言中，摊销并不能有效地与持久性相结合。

但现在来考虑一下按需调用（call-by-need）（比如带记忆的惰性计算）。
如果 `x` 包含被 `f` 需要的中止组件，那么第一个对 `x` 的 `f` 操作将推动组件的求值（昂贵的），并将其记忆。
接下来的操作就可能直接访问被记忆的结果了。
这正是理想的行为。

备注：在回顾中，惰性计算和摊销之间的关系并不令人惊讶。
惰性计算可以被看成自我修正的一种形式，
而摊销经常包含自我修正。
然而，惰性计算是一种特别的自我修正形式————
并不是所有用在摊销非持久性数据结构中的自我修正形式能被重写成惰性计算。
尤其展开（splaying） [ST85] 并不适用这种技术。

###<a name="3-3-2">3.3.2 一个用来分析惰性数据结构的框架</a>

我们刚刚展示了惰性计算在实现纯函数式摊销数据结构中是必要的。
不幸的是，涉及到惰性计算的运行时间分析是出了名地难的。
历史上，最常用的分析惰性程序的技术是假装他们其实是严格计算的。
然而中技巧是完全不足以支持惰性摊销数据结构分析的。
我们将把银行家方法和物理方法进行升级到这个框架，
产生出第一个分析持久性摊销数据结构，也是第一个可行地分析非平凡惰性程序的方法。

我们把任意给定的操作的成本分成几类。
首先，一个操作的非共享成本，是它要执行操作的实际时间，
前提是每一个系统中的中止在操作开始的时候都已经被推动执行而且被记忆了
（即，假设推动执行总是需要 `O(1)` 的时间，除了那些在同一个操作中被创建和推动的中止）。
一个操作的共享成本是，那些在操作中创建但不求值的悬停需要花去的执行时间（与上面假设相同）。
一个操作的完整成本是共享成本和非共享成本的综合。
要注意的是，总成本是把惰性计算换成严格计算情况下，操作的实际成本。

我们进一步把一系列操作的总共享成本划分成为，已实现成本和为实现成本。
已实现成本是在整个计算中被执行的中止的共享成本。
未实现成本是不被执行的中止的共享成本。
一系列操作的总实际成本是非共享成本和已实现共享成本的和————
为实现成本并不能算进实际成本中。
要注意，任意特定操作贡献的成本至少是它的非共享成本，至多是它的总成本，
取决于它多少共享成本被实现了。

我们用累计债务的概念来解释共享成本。
一开始，累计债务是零，但每有一个中止被创建，
我们将累计债务增加中止的共享成本（包括任意的嵌套中止）。
每一个操作随后支付累计债务的一部分。
一个操作的摊销成本是它的非共享成本加上它所支付的部分债务。
只有直到伴随中止的债务完全被还清，我们才能够去推动它。
这种处理债务的方法让人想起预付款，
保留一个商品然后定期付款，但只有全部付清之后才能拿到商品。

一个中止有三个重要的时间节点： 中止被创建的时候，中止被完全支付的时候，和中止被执行的时候。
证明要展示的是第二个时间节点一定在第三个之前。
如果每一个中止都在它被推动之前被完全支付，
那么被支付的债务的总量就是已实现共享成本的上界，
因此，总的摊销成本（即总非共享成本加上总的被支付的债务）是总实际成本的上界
（即总非共享成本加上已实现共享成本）。
我们在3.4.1正式讨论这一点。

分析惰性程序运行时间的一个最困难的问题是推理多个逻辑未来的相互作用。
我们通过假装每一个逻辑未来是唯一的一个逻辑未来，来绕过这个问题进行推理。
从创建中止的操作的角度来看，任意推动这个中止的逻辑未来必须自己支付这个中止的债务。
如果有两个逻辑未来想要推动同一个中止，那么他们就必须各自支付中止的债务。
他们没办法一起各做，各自支付债务的一部分。
这种限制的另一个观点是我们只有当中止的债务在本操作的逻辑历史中被支付才能够推动它。
通过使用这种方法，我们有时候会多次支付同一个债务，
由此高估了特定计算的总耗时，但这并没有太大关系，只是为了简单结果分析付出的一点代价。

##<a name="3-4">银行家方法</a>

我们通过把分数替换成债务来改进银行家方法计算累计债务而不是累计储蓄。
每一份债务代表了中止任务的一个常量，
当我们一开始中止一个给定的计算的时候，
我们按照它的共享成本成比例地创建多个债务，
并将每一个债务关联到对象的一个位置上。
每一个债务的位置的选择决定于计算本身的性质。
如果计算是不可分割的（一旦开始，就执行直到结束），
那么所有的债务一般就关联到结果的根上。
另一方面，如果计算是增量的（即分解成可独立执行的碎片），
那么债务就可能分布在各个部分结果的根上。
